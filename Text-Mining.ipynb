{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.92\n",
      "Test Error = 0.08\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark\") \\\n",
    "    .getOrCreate()\n",
    "#讀檔案\n",
    "#dataRDD =spark.sparkContext.textFile(\"dataset/simpleText.txt\")\\\n",
    "# .map(lambda line: line.split(\"\\t\"))\\\n",
    "# .map(lambda part: Row(label=float(part[0]), text=part[1]))\n",
    "#dataDF = spark.createDataFrame(dataRDD)\n",
    "data = spark.read.option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"delimiter\", \"|\")\\\n",
    "    .csv(\"hotel_review_tokens_tw.txt\")\n",
    "# 訓練 測試 資料集\n",
    "(train, test) = data.randomSplit([0.9,0.1 ], 1234)\n",
    "#斷詞\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer,HashingTF, IDF\n",
    "regexTokenizer = RegexTokenizer().\\\n",
    "    setInputCol(\"raw_words\").\\\n",
    "    setOutputCol(\"words\").\\\n",
    "    setPattern(\" \")\n",
    "#去除停用詞\n",
    "#CountVectorizer詞頻\n",
    "#最多幾個字詞? 取最高頻的\n",
    "#統計字詞大於等於2次\n",
    "tf = CountVectorizer().\\\n",
    "    setInputCol(\"words\").\\\n",
    "    setOutputCol(\"tf_vector\").\\\n",
    "    setVocabSize(30000).\\\n",
    "    setMinDF(1)\n",
    "#HashingTF詞頻\n",
    "hashingTF = HashingTF().\\\n",
    "    setInputCol(\"words\").\\\n",
    "    setOutputCol(\"htf_vector\").\\\n",
    "    setNumFeatures(10000)\n",
    "idf = IDF().\\\n",
    "    setInputCol(\"htf_vector\").\\\n",
    "    setOutputCol(\"htfidf_vector\")\n",
    "\n",
    "#搭配一個你想要的分類器MPL\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "layers = [10000, 80, 40, 2]\n",
    "trainer=MultilayerPerceptronClassifier()\\\n",
    "    .setLayers(layers)\\\n",
    "    .setLabelCol(\"label\")\\\n",
    "    .setFeaturesCol(\"htfidf_vector\")\\\n",
    "    .setPredictionCol(\"prediction\")\\\n",
    "    .setBlockSize(128)\\\n",
    "    .setSeed(1234)\\\n",
    "    .setMaxIter(500)\n",
    "# 這樣寫也可以\n",
    "#trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers,\n",
    "# labelCol=\"label\", featuresCol=\"tfidf_vector\" , blockSize=128, seed=1234)\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, tf, hashingTF, idf, trainer])\n",
    "#pipeline = Pipeline().setStages([regexTokenizer, remover, tf, hashingTF, idf, trainer])\n",
    "#進行訓練與測試\n",
    "model = pipeline.fit(train)\n",
    "result = model.transform(test)\n",
    "#準確率\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",\n",
    "predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(result)\n",
    "print(\"Test Accuracy = %.2f\" % accuracy)\n",
    "print(\"Test Error = %.2f\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請輸入一段文字:不愉快\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\Sam\\Desktop\\machine learning\\jieba\\jieba_big_chinese_dict\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\Sam\\AppData\\Local\\Temp\\jieba.u22256f6af0add31db41b150801fcc58d.cache\n",
      "Loading model cost 0.947 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正向機率:0.0\n",
      "負向機率:1.0\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "review = input(\"請輸入一段文字:\")\n",
    "jieba.set_dictionary('jieba/jieba_big_chinese_dict/dict.txt.big')\n",
    "\n",
    "tokens = []\n",
    "seg_list = jieba.lcut(str(review), cut_all=False)\n",
    "tokens.append( \" \".join(seg_list ))\n",
    "#看切詞\n",
    "tokens\n",
    "#建立新的DataFrame\n",
    "newdata = spark.createDataFrame( [tokens] ,['raw_words'])\n",
    "#模型訓練\n",
    "result = model.transform(newdata)\n",
    "#模型預測\n",
    "prob = result.select(['probability']).toPandas()['probability'].iloc[0].tolist()\n",
    "print(\"正向機率:\"+str(round(prob[1],2)))\n",
    "print(\"負向機率:\"+str(round(prob[0],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
